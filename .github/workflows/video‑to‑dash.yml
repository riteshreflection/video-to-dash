name: Convert uploaded video to DASH (5‑file single‑segment)

###############################################################################
# Triggers
###############################################################################
on:
  repository_dispatch:
    types: [video_uploaded]

  workflow_dispatch:
    inputs:
      publicUrl:
        description: "Direct HTTPS URL to the source MP4"
        required: true
      key:
        description: "Destination key in R2 (optional, e.g. uploads/name.mp4)"
        required: false

###############################################################################
# Global ENV
###############################################################################
env:
  R2_BUCKET: classes
  ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
  R2_ENDPOINT: https://300ec3f71b1ad69fea75564497e04614.r2.cloudflarestorage.com

###############################################################################
# Job
###############################################################################
jobs:
  transcode:
    runs-on: ubuntu-latest
    environment: CloudFlare Secretary          # your secrets live here
    timeout-minutes: 120

    steps:
    # 1️⃣ Checkout (empty pull)
    - name: Checkout
      uses: actions/checkout@v4
      with:
        sparse-checkout: ""

    # 2️⃣ Derive SOURCE_URL and KEY safely
    - name: Derive SOURCE_URL and KEY
      id: derive
      env:
        PAYLOAD_URL: ${{ github.event.client_payload.publicUrl }}
        PAYLOAD_KEY: ${{ github.event.client_payload.key }}
        MANUAL_URL:  ${{ github.event.inputs.publicUrl }}
        MANUAL_KEY:  ${{ github.event.inputs.key }}
      run: |
        set -euo pipefail
        # Prefer repository_dispatch payload, else manual input
        SOURCE_URL="${PAYLOAD_URL:-}"
        [ -z "$SOURCE_URL" ] && SOURCE_URL="${MANUAL_URL}"

        KEY="${PAYLOAD_KEY:-}"
        if [ -z "$KEY" ]; then
          KEY="$MANUAL_KEY"
        fi
        # Final fallback: build a key from the URL filename
        if [ -z "$KEY" ]; then
          KEY="uploads/$(basename "$SOURCE_URL")"
        fi

        echo "SOURCE_URL=$SOURCE_URL" >> $GITHUB_ENV
        echo "KEY=$KEY"               >> $GITHUB_ENV

    # 3️⃣ Install FFmpeg
    - name: Install FFmpeg
      run: |
        sudo apt-get update -y
        sudo apt-get install -y ffmpeg
        ffmpeg -version | head -n1

    # 4️⃣ Cache + install AWS CLI (fast on re‑run)
    - name: Cache pip dir for AWS CLI
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: pip-awscli-v2

    - name: Install AWS CLI
      run: |
        python3 -m pip install --upgrade --user awscli
        echo "$HOME/.local/bin" >> $GITHUB_PATH
        aws --version

    # 5️⃣ Download the video
    - name: Download input MP4
      run: |
        mkdir -p work
        echo "Downloading: $SOURCE_URL"
        curl -L "$SOURCE_URL" --fail -o work/input.mp4
        du -h work/input.mp4

    # 6️⃣ Transcode → 5‑file single‑segment DASH (720/480/360 + audio)
    - name: Convert to DASH (5 files)
      run: |
        mkdir -p work/dash

        CLEAN_NAME=$(basename "$KEY" .mp4)   # remove folders & .mp4
        [ -z "$CLEAN_NAME" ] && CLEAN_NAME="video_$(date +%s)"
        BASE="$CLEAN_NAME"

        ffmpeg -y -i work/input.mp4 \
          \
          # --- 720p ---------------------------------------------------------
          -map 0:v -c:v:0 libx264 -b:v:0 1500k -maxrate:v:0 1600k -bufsize:v:0 2000k \
          -vf:0 scale=w=1280:h=-2:flags=bicubic \
          \
          # --- 480p ---------------------------------------------------------
          -map 0:v -c:v:1 libx264 -b:v:1 800k  -maxrate:v:1 900k  -bufsize:v:1 1200k \
          -vf:1 scale=w=854:h=-2:flags=bicubic \
          \
          # --- 360p ---------------------------------------------------------
          -map 0:v -c:v:2 libx264 -b:v:2 400k  -maxrate:v:2 450k  -bufsize:v:2 600k \
          -vf:2 scale=w=640:h=-2:flags=bicubic \
          \
          # --- audio --------------------------------------------------------
          -map 0:a -c:a aac -b:a 128k \
          \
          -f dash \
          -single_file 1 \
          -min_seg_duration 5000000 \
          -adaptation_sets "id=0,streams=v id=1,streams=a" \
          "work/dash/${BASE}.mpd"

        echo "Files generated:"
        ls -lh work/dash

    # 7️⃣ Upload 5 objects → R2
    - name: Upload to R2
      env:
        AWS_ACCESS_KEY_ID:     ${{ secrets.R2_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        AWS_EC2_METADATA_DISABLED: true
      run: |
        aws s3 cp work/dash "s3://${{ env.R2_BUCKET }}/dash/" --recursive \
          --endpoint-url "${{ env.R2_ENDPOINT }}"

    # 8️⃣ Summary
    - name: Report summary
      run: |
        echo "### 🎉 DASH manifest" >> $GITHUB_STEP_SUMMARY
        echo "https://classes.thecampus.in/dash/${CLEAN_NAME}.mpd" >> $GITHUB_STEP_SUMMARY
