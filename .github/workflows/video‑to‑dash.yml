name: Convert uploaded video to DASH â€‘ singleâ€‘file (720/480/360)

on:
  # automatic trigger from the Worker
  repository_dispatch:
    types: [video_uploaded]

  # manual trigger in GitHubÂ UI
  workflow_dispatch:
    inputs:
      publicUrl:
        description: "Direct URL to the source MP4 (https://.../file.mp4)"
        required: true
      key:
        description: "Destination key in R2 (optional, e.g. uploads/my.mp4)"
        required: false

###############################################################################
# Global ENV â€“Â account id comes from the environmentâ€‘level secret you created
###############################################################################
env:
  R2_BUCKET: classes
  ACCOUNT_ID: ${{ secrets.ACCOUNT_ID }}
  R2_ENDPOINT: https://300ec3f71b1ad69fea75564497e04614.r2.cloudflarestorage.com

jobs:
  transcode:
    runs-on: ubuntu-latest
    environment: CloudFlare Secretary          # << your secrets live here
    timeout-minutes: 120

    steps:
    # 1ï¸âƒ£Â checkout (empty)
    - name: Checkout (empty)
      uses: actions/checkout@v4
      with:
        sparse-checkout: ""

    # 2ï¸âƒ£Â derive SOURCE_URL and KEY (cannot use || inside ${{ }})
    - name: Derive SOURCE_URL and KEY
      id: derive
      run: |
        set -euo pipefail
        # read from dispatch payload
        SOURCE_URL="${{ github.event.client_payload.publicUrl }}"
        KEY="${{ github.event.client_payload.key }}"

        # fall back to manual inputs when undefined
        if [ -z "$SOURCE_URL" ]; then
          SOURCE_URL="${{ github.event.inputs.publicUrl }}"
        fi
        if [ -z "$KEY" ]; then
          KEY_INPUT="${{ github.event.inputs.key }}"
          if [ -n "$KEY_INPUT" ]; then
            KEY="$KEY_INPUT"
          else
            KEY="uploads/$(basename "$SOURCE_URL")"
          fi
        fi

        echo "SOURCE_URL=$SOURCE_URL" >> $GITHUB_ENV
        echo "KEY=$KEY"               >> $GITHUB_ENV
        echo "Using SOURCE_URL=$SOURCE_URL"
        echo "Using KEY=$KEY"

    # 3ï¸âƒ£Â install FFmpeg
    - name: Install FFmpeg
      run: |
        sudo apt-get update -y
        sudo apt-get install -y ffmpeg
        ffmpeg -version | head -n1

    # 4ï¸âƒ£Â install AWS CLI (pip)
    - name: Install AWS CLI
      run: |
        sudo apt-get update -y
        sudo apt-get install -y python3-pip
        python3 -m pip install --upgrade --user awscli
        echo "$HOME/.local/bin" >> $GITHUB_PATH
        aws --version

    # 5ï¸âƒ£Â download source
    - name: Download original MP4
      run: |
        mkdir -p work
        echo "Downloading $SOURCE_URL"
        curl -L "$SOURCE_URL" --fail -o work/input.mp4
        du -h work/input.mp4

    # 6ï¸âƒ£Â transcode to DASH singleâ€‘file, 3 video reps + 1 audio  â†’ 5 objects
    - name: Convert to DASH (singleâ€‘file, 720/480/360 + audio)
      run: |
        mkdir -p work/dash
        BASE=$(basename "$KEY" .mp4)

        ffmpeg -y -i work/input.mp4 \
          \
          # --- 720p ---------------------------------------------------------
          -map 0:v -c:v:0 libx264 -b:v:0 1500k -maxrate:v:0 1600k -bufsize:v:0 2000k \
          -vf:0 scale=w=1280:h=-2:flags=bicubic \
          \
          # --- 480p ---------------------------------------------------------
          -map 0:v -c:v:1 libx264 -b:v:1 800k  -maxrate:v:1 900k  -bufsize:v:1 1200k \
          -vf:1 scale=w=854:h=-2:flags=bicubic \
          \
          # --- 360p ---------------------------------------------------------
          -map 0:v -c:v:2 libx264 -b:v:2 400k  -maxrate:v:2 450k  -bufsize:v:2 600k \
          -vf:2 scale=w=640:h=-2:flags=bicubic \
          \
          # --- audio (128Â kbps) --------------------------------------------
          -map 0:a -c:a aac -b:a 128k \
          \
          -f dash \
          -single_file 1 \
          -min_seg_duration 5000000 \
          -adaptation_sets "id=0,streams=v id=1,streams=a" \
          work/dash/${BASE}.mpd

        echo "Files generated:"
        ls -lh work/dash

    # 7ï¸âƒ£Â upload to R2 (5 objects â†’ 5 writes)
    - name: Upload DASH to R2
      env:
        AWS_ACCESS_KEY_ID:     ${{ secrets.R2_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        AWS_EC2_METADATA_DISABLED: true
      run: |
        aws s3 cp work/dash "s3://${{ env.R2_BUCKET }}/dash/" --recursive \
          --endpoint-url "${{ env.R2_ENDPOINT }}"

    # 8ï¸âƒ£Â summary
    - name: Report summary
      run: |
        BASE=$(basename "$KEY" .mp4)
        echo "ðŸŽ‰Â DASH manifest ready at:" >> $GITHUB_STEP_SUMMARY
        echo "https://classes.thecampus.in/dash/${BASE}.mpd" >> $GITHUB_STEP_SUMMARY
